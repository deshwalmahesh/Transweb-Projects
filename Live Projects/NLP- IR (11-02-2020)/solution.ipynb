{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='english') # official documentation says SnowBall stemmer is better than others for Eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(x,descending=True):\n",
    "    '''\n",
    "    method to return a sorted dict based on the values for. for example {'a':50,'b':2,'c':10} would be returned as either\n",
    "    {'a':50,'c':10,'b':2} or {'b':2,'c':10,'a':50}\n",
    "    args:\n",
    "        x: dictionary\n",
    "        descending: whether to return a descending or ascending dict\n",
    "    out:\n",
    "        sorted dict based on values\n",
    "    '''\n",
    "    assert len(x)>1, 'Single Element Dictionary'\n",
    "    return {k: v for k, v in sorted(x.items(), key=lambda item: item[1],reverse=descending)}\n",
    "\n",
    "\n",
    "def get_vocab(sent_tok_list):\n",
    "    '''\n",
    "    Function to build a vicablary in form of {'word':frequency} dict\n",
    "    param:\n",
    "        text_list: a list of all the word tokens in the document\n",
    "    out:\n",
    "        vocab: a dictonary with words as keys and respective frequencies as values\n",
    "        '''\n",
    "    vocab = {} # dict that will be returned\n",
    "    for sent_tokens in sent_tok_list:\n",
    "        for word in sent_tokens:\n",
    "            word = word.lower().strip()\n",
    "            if word not in sw and word not in punctuation:\n",
    "                # could use if vocab[word] in vocab then +=1 else 1 \n",
    "                # or could have imported the DeefaultDict\n",
    "                word = stemmer.stem(word)\n",
    "                try:\n",
    "                    vocab[word] += 1\n",
    "                except KeyError:\n",
    "                    vocab[word] = 1\n",
    "                    \n",
    "    return sort_dict(vocab)\n",
    "\n",
    "\n",
    "def inverted_index(sent_token_list):\n",
    "    IF_score = {}\n",
    "    for DocId, doc in enumerate(sent_token_list):\n",
    "        for word in doc:\n",
    "            word = word.lower().strip()\n",
    "            if (word not in sw and word not in punctuation):\n",
    "                word = stemmer.stem(word)\n",
    "                try:\n",
    "                    if DocId+1 in IF_score[word]:\n",
    "                        IF_score[word][DocId+1]+=1\n",
    "                    else:\n",
    "                        IF_score[word][DocId+1] = 1\n",
    "                except KeyError:\n",
    "                    IF_score[word] = {}\n",
    "                    IF_score[word][DocId+1] = 1\n",
    "    return IF_score\n",
    "\n",
    "\n",
    "def cleaned_docs(sent_tok_list):\n",
    "    '''\n",
    "    Function to build a vicablary in form of {'word':frequency} dict\n",
    "    param:\n",
    "        text_list: a list of all the word tokens in the document\n",
    "    out:\n",
    "        vocab: a dictonary with words as keys and respective frequencies as values\n",
    "        '''\n",
    "    cleaned_doc_list = [] # dict that will be returned\n",
    "    for sent_tokens in sent_tok_list:\n",
    "        doc = []\n",
    "        for word in sent_tokens:\n",
    "            word = word.lower().strip()\n",
    "            if word not in sw and word not in punctuation:\n",
    "                word = stemmer.stem(word)\n",
    "                doc.append(word)\n",
    "                \n",
    "        cleaned_doc_list.append(' '.join(doc))\n",
    "    return cleaned_doc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = [i.lower().strip() for i in 'Is, An, That, Use, And, To, From, In, Both, Of, At, The'.split(',')] # stop words\n",
    "doc_1 = 'data science is a field to use scientific method, process, algorithm, system to extract knowledge.'\n",
    "doc_2 = 'data mining is the process to discover pattern in large data to involve method at the database system.'\n",
    "doc_3 = 'information system is the study of network of hardware and software that people use to process data.'\n",
    "DOC = [doc_1,doc_2,doc_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_TOKENS = [word_tokenize(sent) for sent in  DOC] # list of list of word tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 1.1, 1.2 \n",
    "Both are implemented via the functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(DOC_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': 4,\n",
       " 'process': 3,\n",
       " 'system': 3,\n",
       " 'method': 2,\n",
       " 'scienc': 1,\n",
       " 'a': 1,\n",
       " 'field': 1,\n",
       " 'scientif': 1,\n",
       " 'algorithm': 1,\n",
       " 'extract': 1,\n",
       " 'knowledg': 1,\n",
       " 'mine': 1,\n",
       " 'discov': 1,\n",
       " 'pattern': 1,\n",
       " 'larg': 1,\n",
       " 'involv': 1,\n",
       " 'databas': 1,\n",
       " 'inform': 1,\n",
       " 'studi': 1,\n",
       " 'network': 1,\n",
       " 'hardwar': 1,\n",
       " 'softwar': 1,\n",
       " 'peopl': 1}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```data``` has came 4 times in all of the documents, ```process``` thrice and ```method``` twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'process', 'system', 'method', 'scienc']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlargest(5,vocab,vocab.get) # get the top 5 occuring words in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = inverted_index(DOC_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {1: 1, 2: 2, 3: 1},\n",
       " 'scienc': {1: 1},\n",
       " 'a': {1: 1},\n",
       " 'field': {1: 1},\n",
       " 'scientif': {1: 1},\n",
       " 'method': {1: 1, 2: 1},\n",
       " 'process': {1: 1, 2: 1, 3: 1},\n",
       " 'algorithm': {1: 1},\n",
       " 'system': {1: 1, 2: 1, 3: 1},\n",
       " 'extract': {1: 1},\n",
       " 'knowledg': {1: 1},\n",
       " 'mine': {2: 1},\n",
       " 'discov': {2: 1},\n",
       " 'pattern': {2: 1},\n",
       " 'larg': {2: 1},\n",
       " 'involv': {2: 1},\n",
       " 'databas': {2: 1},\n",
       " 'inform': {3: 1},\n",
       " 'studi': {3: 1},\n",
       " 'network': {3: 1},\n",
       " 'hardwar': {3: 1},\n",
       " 'softwar': {3: 1},\n",
       " 'peopl': {3: 1}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```data``` came once in document 1, twice in document 2 and once in document 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q. 1.4\n",
    "Boolean Queries. Construct queries which apperar in atleast 2 documents using and,or,not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = 'data science is a field to use scientific method, process, algorithm, system to extract knowledge.'\n",
    "doc_2 = 'data mining is the process to discover pattern in large data to involve method at the database system.'\n",
    "doc_3 = 'information system is the study of network of hardware and software that people use to process data.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2 = ('data' or 'method') and ('database') # return document 2\n",
    "d_1 = not('pattern' or 'mine') or ('data' or 'method' ) # returns document 1\n",
    "d_3 = not('data' or 'method') or (('system' or 'process') or ('inform' or 'network'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 1.5 Vector Model (using TfiDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_DOC = cleaned_docs(DOC_TOKENS) # document with cleaned words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer().fit(CLEANED_DOC) # fit in the the document data only\n",
    "tf_docs = tf_vect.transform(CLEANED_DOC) # transform the document data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'data science and algorithm is fun while mining data to discover pattern in data and applying things'\n",
    "query = list(get_vocab([word_tokenize(query)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93818073, 1.41990924, 0.22249441])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cosine_similarity(tf_vect.transform(query).todense(),tf_docs.todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means that the given query is most relevent with the ```Second``` document"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
